{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reference: https://github.com/deepseasw/seq2seq_chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers, losses, metrics\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "\n",
    "from konlpy.tag import Mecab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seq에서의 임베딩이 이전 예제와 다른 점은 아래와 같이 태그를 사용한다는 것입니다. 임베딩의 0~3번째에 각각 PADDING, START, END, OOV 태그를 넣습니다. 사실 그냥 똑같은 단어라고 보시면 됩니다. 다만 이 단어들이 Seq2Seq의 동작을 제어합니다. \n",
    "\n",
    "예를 들어, 디코더 입력에 START가 들어가면 디코딩의 시작을 의미합니다. 반대로 디코더 출력에 END가 나오면 디코딩을 종료합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 태그 단어\n",
    "PAD = \"<PADDING>\"   # 패딩\n",
    "STA = \"<START>\"     # 시작\n",
    "END = \"<END>\"       # 끝\n",
    "OOV = \"<OOV>\"       # 없는 단어(Out of Vocabulary)\n",
    "\n",
    "# 태그 인덱스\n",
    "PAD_INDEX = 0\n",
    "STA_INDEX = 1\n",
    "END_INDEX = 2\n",
    "OOV_INDEX = 3\n",
    "\n",
    "# 데이터 타입\n",
    "ENCODER_INPUT  = 0\n",
    "DECODER_INPUT  = 1\n",
    "DECODER_TARGET = 2\n",
    "\n",
    "# 한 문장에서 단어 시퀀스의 최대 개수\n",
    "max_sequences = 30\n",
    "\n",
    "# 임베딩 벡터 차원\n",
    "embedding_dim = 100\n",
    "\n",
    "# LSTM 히든레이어 차원\n",
    "lstm_hidden_dim = 128\n",
    "\n",
    "# 정규 표현식 필터\n",
    "RE_FILTER = re.compile(\"[.,!?\\\"':;~()]\")\n",
    "\n",
    "# 챗봇 데이터 로드\n",
    "chatbot_data = pd.read_csv('./dataset/chatbot/ChatbotData.csv', encoding='utf-8')\n",
    "question, answer = list(chatbot_data['Q']), list(chatbot_data['A'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "챗봇의 훈련을 위해서 송영숙님이 공개한 한글 데이터셋을 로드합니다. 질문과 대답, 감정 등 총 3개의 항목으로 구성되어 있습니다. 감정 분류는 Seq2Seq에 필요가 없기 때문에 사용하지 않습니다.\n",
    "\n",
    "https://github.com/songys/Chatbot_data\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11823"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 개수\n",
    "len(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# 데이터의 일부만 학습에 사용\n",
    "question = question[:100]\n",
    "answer = answer[:100]\n",
    "\n",
    "# 챗봇 데이터 출력\n",
    "for i in range(10):\n",
    "    print('Q : ' + question[i])\n",
    "    print('A : ' + answer[i])\n",
    "    print()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q : 12시 땡!\n",
      "A : 하루가 또 가네요.\n",
      "\n",
      "Q : 1지망 학교 떨어졌어\n",
      "A : 위로해 드립니다.\n",
      "\n",
      "Q : 3박4일 놀러가고 싶다\n",
      "A : 여행은 언제나 좋죠.\n",
      "\n",
      "Q : 3박4일 정도 놀러가고 싶다\n",
      "A : 여행은 언제나 좋죠.\n",
      "\n",
      "Q : PPL 심하네\n",
      "A : 눈살이 찌푸려지죠.\n",
      "\n",
      "Q : SD카드 망가졌어\n",
      "A : 다시 새로 사는 게 마음 편해요.\n",
      "\n",
      "Q : SD카드 안돼\n",
      "A : 다시 새로 사는 게 마음 편해요.\n",
      "\n",
      "Q : SNS 맞팔 왜 안하지ㅠㅠ\n",
      "A : 잘 모르고 있을 수도 있어요.\n",
      "\n",
      "Q : SNS 시간낭비인 거 아는데 매일 하는 중\n",
      "A : 시간을 정하고 해보세요.\n",
      "\n",
      "Q : SNS 시간낭비인데 자꾸 보게됨\n",
      "A : 시간을 정하고 해보세요.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전부 학습에 사용!!\n",
    "# 챗봇 데이터 출력\n",
    "for i in range(10):\n",
    "    print('Q : ' + question[i])\n",
    "    print('A : ' + answer[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 사전 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소분석 함수\n",
    "def pos_tag(sentences):\n",
    "    \n",
    "    # KoNLPy 형태소분석기 설정\n",
    "    tagger = Mecab()\n",
    "    \n",
    "    # 문장 품사 변수 초기화\n",
    "    sentences_pos = []\n",
    "    \n",
    "    # 모든 문장 반복\n",
    "    for sentence in sentences:\n",
    "        # 특수기호 제거\n",
    "        sentence = re.sub(RE_FILTER, \"\", sentence)\n",
    "        \n",
    "        # 배열인 형태소분석의 출력을 띄어쓰기로 구분하여 붙임\n",
    "        sentence = \" \".join(tagger.morphs(sentence))\n",
    "        sentences_pos.append(sentence)\n",
    "        \n",
    "    return sentences_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장을 먼저 최소 단위인 토큰으로 나누어야 합니다. 한글 문장이라면 보통 KoNLPy(코엔엘파이)를 통해 형태소분석을 적용합니다. Mecab 외에 Hannanum, Kkma, Komoran, Mecab, Okt(Twitter) 등 여러가지 라이브러리를 동일한 인터페이스로 실행이 가능합니다.\n",
    "\n",
    "KoNLPy는 자바로 구현이 되어 있습니다. 맥OS와 달리 윈도우 아나콘다는 아래와 같은 작업을 거쳐야 합니다.\n",
    "\n",
    "< 1. 자바 설치 ><br>\n",
    "커맨드창에서 'java -version' 명령어를 실행하여 버전이 1.7 이상이어야 합니다.\n",
    "\n",
    "< 2. JPype1 설치 ><br>\n",
    "파이썬에서 자바를 호출할 수 있는 라이브러리입니다.\n",
    "\n",
    "< 3. KoNLPy 설치 ><br>\n",
    "위의 두 가지가 미리 완료되어 있어야만 가능합니다.\n",
    "\n",
    "아래 링크에서 좀 더 자세한 방법을 확인하시기 바랍니다.<br>\n",
    "-> https://ericnjennifer.github.io/python_visualization/2018/01/21/PythonVisualization_Chapt1.html\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q : 12 시 땡\n",
      "A : 하루 가 또 가 네요\n",
      "\n",
      "Q : 1 지망 학교 떨어졌 어\n",
      "A : 위로 해 드립니다\n",
      "\n",
      "Q : 3 박 4 일 놀 러 가 고 싶 다\n",
      "A : 여행 은 언제나 좋 죠\n",
      "\n",
      "Q : 3 박 4 일 정도 놀 러 가 고 싶 다\n",
      "A : 여행 은 언제나 좋 죠\n",
      "\n",
      "Q : PPL 심하 네\n",
      "A : 눈살 이 찌푸려 지 죠\n",
      "\n",
      "Q : SD 카드 망가졌 어\n",
      "A : 다시 새로 사 는 게 마음 편해요\n",
      "\n",
      "Q : SD 카드 안 돼\n",
      "A : 다시 새로 사 는 게 마음 편해요\n",
      "\n",
      "Q : SNS 맞 팔 왜 안 하 지 ㅠㅠ\n",
      "A : 잘 모르 고 있 을 수 도 있 어요\n",
      "\n",
      "Q : SNS 시간 낭비 인 거 아 는데 매일 하 는 중\n",
      "A : 시간 을 정하 고 해 보 세요\n",
      "\n",
      "Q : SNS 시간 낭비 인데 자꾸 보 게 됨\n",
      "A : 시간 을 정하 고 해 보 세요\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 형태소분석 수행\n",
    "question = pos_tag(question)\n",
    "answer = pos_tag(answer)\n",
    "\n",
    "# 형태소분석으로 변환된 챗봇 데이터 출력\n",
    "for i in range(10):\n",
    "    print('Q : ' + question[i])\n",
    "    print('A : ' + answer[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문과 대답 문장들을 하나로 합침\n",
    "sentences = []\n",
    "sentences.extend(question)\n",
    "sentences.extend(answer)\n",
    "\n",
    "words = []\n",
    "\n",
    "# 단어들의 배열 생성\n",
    "for sentence in sentences:\n",
    "    for word in sentence.split():\n",
    "        words.append(word)\n",
    "\n",
    "# 길이가 0인 단어는 삭제\n",
    "words = [word for word in words if len(word) > 0]\n",
    "\n",
    "# 중복된 단어 삭제\n",
    "words = list(set(words))\n",
    "\n",
    "# 제일 앞에 태그 단어 삽입\n",
    "words[:0] = [PAD, STA, END, OOV]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "질문과 대답 문장들을 합쳐서 전체 단어 사전을 만듭니다. 자연어처리에서는 항상 이렇게 단어를 인덱스에 따라 정리를 해야 합니다. 그래야지 문장을 인덱스 배열로 바꿔서 임베딩 레이어에 넣을 수 있습니다. 또한 모델의 출력에서 나온 인덱스를 다시 단어로 변환하는데도 필요합니다.\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6852"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 개수\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<PADDING>',\n",
       " '<START>',\n",
       " '<END>',\n",
       " '<OOV>',\n",
       " '설마',\n",
       " '그런지',\n",
       " '마냥',\n",
       " '아재',\n",
       " '멋',\n",
       " '내편',\n",
       " '강림',\n",
       " '관계',\n",
       " '소분',\n",
       " '드나',\n",
       " '둬서',\n",
       " '민감',\n",
       " '중요',\n",
       " '갔',\n",
       " '쓰디쓰',\n",
       " '성적']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 출력\n",
    "words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어와 인덱스의 딕셔너리 생성\n",
    "word_to_index = {word: index for index, word in enumerate(words)}\n",
    "index_to_word = {index: word for index, word in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PADDING>': 0,\n",
       " '<START>': 1,\n",
       " '<END>': 2,\n",
       " '<OOV>': 3,\n",
       " '설마': 4,\n",
       " '그런지': 5,\n",
       " '마냥': 6,\n",
       " '아재': 7,\n",
       " '멋': 8,\n",
       " '내편': 9,\n",
       " '강림': 10,\n",
       " '관계': 11,\n",
       " '소분': 12,\n",
       " '드나': 13,\n",
       " '둬서': 14,\n",
       " '민감': 15,\n",
       " '중요': 16,\n",
       " '갔': 17,\n",
       " '쓰디쓰': 18,\n",
       " '성적': 19}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 -> 인덱스\n",
    "# 문장을 인덱스로 변환하여 모델 입력으로 사용\n",
    "dict(list(word_to_index.items())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<PADDING>',\n",
       " 1: '<START>',\n",
       " 2: '<END>',\n",
       " 3: '<OOV>',\n",
       " 4: '설마',\n",
       " 5: '그런지',\n",
       " 6: '마냥',\n",
       " 7: '아재',\n",
       " 8: '멋',\n",
       " 9: '내편',\n",
       " 10: '강림',\n",
       " 11: '관계',\n",
       " 12: '소분',\n",
       " 13: '드나',\n",
       " 14: '둬서',\n",
       " 15: '민감',\n",
       " 16: '중요',\n",
       " 17: '갔',\n",
       " 18: '쓰디쓰',\n",
       " 19: '성적'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인덱스 -> 단어\n",
    "# 모델의 예측 결과인 인덱스를 문장으로 변환시 사용\n",
    "dict(list(index_to_word.items())[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 인덱스로 변환\n",
    "def convert_text_to_index(sentences, vocabulary, type): \n",
    "    \n",
    "    sentences_index = []\n",
    "    \n",
    "    # 모든 문장에 대해서 반복\n",
    "    for sentence in sentences:\n",
    "        sentence_index = []\n",
    "        \n",
    "        # 디코더 입력일 경우 맨 앞에 START 태그 추가\n",
    "        if type == DECODER_INPUT:\n",
    "            sentence_index.extend([vocabulary[STA]])\n",
    "        \n",
    "        # 문장의 단어들을 띄어쓰기로 분리\n",
    "        for word in sentence.split():\n",
    "            if vocabulary.get(word) is not None:\n",
    "                # 사전에 있는 단어면 해당 인덱스를 추가\n",
    "                sentence_index.extend([vocabulary[word]])\n",
    "            else:\n",
    "                # 사전에 없는 단어면 OOV 인덱스를 추가\n",
    "                sentence_index.extend([vocabulary[OOV]])\n",
    "\n",
    "        # 최대 길이 검사\n",
    "        if type == DECODER_TARGET:\n",
    "            # 디코더 목표일 경우 맨 뒤에 END 태그 추가\n",
    "            if len(sentence_index) >= max_sequences:\n",
    "                sentence_index = sentence_index[:max_sequences-1] + [vocabulary[END]]\n",
    "            else:\n",
    "                sentence_index += [vocabulary[END]]\n",
    "        else:\n",
    "            if len(sentence_index) > max_sequences:\n",
    "                sentence_index = sentence_index[:max_sequences]\n",
    "            \n",
    "        # 최대 길이에 없는 공간은 패딩 인덱스로 채움\n",
    "        sentence_index += (max_sequences - len(sentence_index)) * [vocabulary[PAD]]\n",
    "        \n",
    "        # 문장의 인덱스 배열을 추가\n",
    "        sentences_index.append(sentence_index)\n",
    "\n",
    "    return np.asarray(sentences_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seq에서는 학습시 다음과 같이 총 3개의 데이터가 필요합니다.\n",
    "\n",
    "인코더 입력 : 12시 땡<br>\n",
    "디코더 입력 : START 하루 가 또 가네요<br>\n",
    "디코더 출력 : 하루 가 또 가네요 END\n",
    "\n",
    "원래 Seq2Seq는 디코더의 현재 출력이 디코더의 다음 입력으로 들어갑니다. 다만 학습에서는 굳이 이렇게 하지 않고 디코더 입력과 디코더 출력의 데이터를 각각 만듭니다. \n",
    "\n",
    "그러나 예측시에는 이런 방식이 불가능합니다. 출력값을 미리 알지 못하기 때문에, 디코더 입력을 사전에 생성할 수가 없습니다. 이런 문제를 해결하기 위해 훈련 모델과 예측 모델을 따로 구성해야 합니다. 모델 생성 부분에서 다시 자세히 설명을 드리겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1854, 2677, 3998,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인코더 입력 인덱스 변환\n",
    "x_encoder = convert_text_to_index(question, word_to_index, ENCODER_INPUT)\n",
    "\n",
    "# 첫 번째 인코더 입력 출력 (12시 땡)\n",
    "x_encoder[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1, 6600, 4109, 4831, 4109, 6628,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 디코더 입력 인덱스 변환\n",
    "x_decoder = convert_text_to_index(answer, word_to_index, DECODER_INPUT)\n",
    "\n",
    "# 첫 번째 디코더 입력 출력 (START 하루 가 또 가네요)\n",
    "x_decoder[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6600, 4109, 4831, 4109, 6628,    2,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 디코더 목표 인덱스 변환\n",
    "y_decoder = convert_text_to_index(answer, word_to_index, DECODER_TARGET)\n",
    "\n",
    "# 첫 번째 디코더 목표 출력 (하루 가 또 가네요 END)\n",
    "y_decoder[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory explosion을 방지하기 위해 `fit_generator` 매서드를 위한 class를 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSequence(Sequence):\n",
    "    def __init__(self, x_encoder, x_decoder, y_decoder):\n",
    "        self.x_encoder = x_encoder\n",
    "        self.x_decoder = x_decoder\n",
    "        self.y_decoder = y_decoder\n",
    "        self.batch_size = 128\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_encoder) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.batch_size\n",
    "        end = start + self.batch_size\n",
    "\n",
    "        # one-hot encoding\n",
    "        x = self.x_encoder[start:end]\n",
    "        y = self.x_decoder[start:end]\n",
    "        target = self.y_decoder[start:end]\n",
    "        \n",
    "        one_hot_data = np.zeros((len(target), max_sequences, len(words)))\n",
    "\n",
    "        # 디코더 목표를 원핫인코딩으로 변환\n",
    "        # 학습시 입력은 인덱스이지만, 출력은 원핫인코딩 형식임\n",
    "        for i, sequence in enumerate(target):\n",
    "            for j, index in enumerate(sequence):\n",
    "                one_hot_data[i, j, index] = 1\n",
    "\n",
    "        # 디코더 목표 설정\n",
    "        target = one_hot_data\n",
    "\n",
    "        return [x, y], target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인코더 입력과 디코더 입력은 임베딩 레이어에 들어가는 인덱스 배열입니다. 반면에 디코더 출력은 원핫인코딩 형식이 되어야 합니다. 디코더의 마지막 Dense 레이어에서 softmax로 나오기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "# 훈련 모델 인코더 정의                       \n",
    "#--------------------------------------------\n",
    "\n",
    "# 입력 문장의 인덱스 시퀀스를 입력으로 받음\n",
    "encoder_inputs = layers.Input(shape=(None,))\n",
    "\n",
    "# 임베딩 레이어\n",
    "encoder_outputs = layers.Embedding(len(words), embedding_dim)(encoder_inputs)\n",
    "\n",
    "# return_state가 True면 상태값 리턴\n",
    "# LSTM은 state_h(hidden state)와 state_c(cell state) 2개의 상태 존재\n",
    "encoder_outputs, state_h, state_c = layers.LSTM(lstm_hidden_dim,\n",
    "                                                dropout=0.1,\n",
    "                                                recurrent_dropout=0.5,\n",
    "                                                return_state=True)(encoder_outputs)\n",
    "\n",
    "# 히든 상태와 셀 상태를 하나로 묶음\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------\n",
    "# 훈련 모델 디코더 정의\n",
    "#--------------------------------------------\n",
    "\n",
    "# 목표 문장의 인덱스 시퀀스를 입력으로 받음\n",
    "decoder_inputs = layers.Input(shape=(None,))\n",
    "\n",
    "# 임베딩 레이어\n",
    "decoder_embedding = layers.Embedding(len(words), embedding_dim)\n",
    "decoder_outputs = decoder_embedding(decoder_inputs)\n",
    "\n",
    "# 인코더와 달리 return_sequences를 True로 설정하여 모든 타임 스텝 출력값 리턴\n",
    "# 모든 타임 스텝의 출력값들을 다음 레이어의 Dense()로 처리하기 위함\n",
    "decoder_lstm = layers.LSTM(lstm_hidden_dim,\n",
    "                           dropout=0.1,\n",
    "                           recurrent_dropout=0.5,\n",
    "                           return_state=True,\n",
    "                           return_sequences=True)\n",
    "\n",
    "# initial_state를 인코더의 상태로 초기화\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_outputs,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "# 단어의 개수만큼 노드의 개수를 설정하여 원핫 형식으로 각 단어 인덱스를 출력\n",
    "decoder_dense = layers.Dense(len(words), activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------\n",
    "# 훈련 모델 정의\n",
    "#--------------------------------------------\n",
    "\n",
    "# 입력과 출력으로 함수형 API 모델 생성\n",
    "model = models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# 학습 방법 설정\n",
    "adam = optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지의 예제는 Sequential 방식의 모델이었습니다. 하지만 이번에는 함수형 API 모델을 사용했습니다. 인코더와 디코더가 따로 분리되어야 하는데, 단순히 레이어를 추가하여 붙이는 순차형으로는 구현이 불가능하기 때문입니다. \n",
    "\n",
    "Model() 함수로 입력과 출력을 따로 설정하여 모델을 만듭니다. 그다음 compile과 fit은 이전과 동일하게 적용하시면 됩니다.\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "#  예측 모델 인코더 정의\n",
    "#--------------------------------------------\n",
    "\n",
    "# 훈련 모델의 인코더 상태를 사용하여 예측 모델 인코더 설정\n",
    "encoder_model = models.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "\n",
    "#--------------------------------------------\n",
    "# 예측 모델 디코더 정의\n",
    "#--------------------------------------------\n",
    "\n",
    "# 예측시에는 훈련시와 달리 타임 스텝을 한 단계씩 수행\n",
    "# 매번 이전 디코더 상태를 입력으로 받아서 새로 설정\n",
    "decoder_state_input_h = layers.Input(shape=(lstm_hidden_dim,))\n",
    "decoder_state_input_c = layers.Input(shape=(lstm_hidden_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]    \n",
    "\n",
    "# 임베딩 레이어\n",
    "decoder_outputs = decoder_embedding(decoder_inputs)\n",
    "\n",
    "# LSTM 레이어\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_outputs,\n",
    "                                                 initial_state=decoder_states_inputs)\n",
    "\n",
    "# 히든 상태와 셀 상태를 하나로 묶음\n",
    "decoder_states = [state_h, state_c]\n",
    "\n",
    "# Dense 레이어를 통해 원핫 형식으로 각 단어 인덱스를 출력\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# 예측 모델 디코더 설정\n",
    "decoder_model = models.Model([decoder_inputs] + decoder_states_inputs,\n",
    "                      [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예측 모델은 이미 학습된 훈련 모델의 레이어들을 그대로 재사용합니다. 예측 모델 인코더는 훈련 모델 인코더과 동일합니다. 그러나 예측 모델 디코더는 매번 LSTM 상태값을 입력으로 받습니다. 또한 디코더의 LSTM 상태를 출력값과 같이 내보내서, 다음 번 입력에 넣습니다. \n",
    "\n",
    "이렇게 하는 이유는 LSTM을 딱 한번의 타임 스텝만 실행하기 때문입니다. 그래서 매번 상태값을 새로 초기화 해야 합니다. 이와 반대로 훈련할때는 문장 전체를 계속 LSTM으로 돌리기 때문에 자동으로 상태값이 전달됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 및 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스를 문장으로 변환\n",
    "def convert_index_to_text(indexs, vocabulary): \n",
    "    \n",
    "    sentence = ''\n",
    "    \n",
    "    # 모든 문장에 대해서 반복\n",
    "    for index in indexs:\n",
    "        if index == END_INDEX:\n",
    "            # 종료 인덱스면 중지\n",
    "            break;\n",
    "        if vocabulary.get(index) is not None:\n",
    "            # 사전에 있는 인덱스면 해당 단어를 추가\n",
    "            sentence += vocabulary[index]\n",
    "        else:\n",
    "            # 사전에 없는 인덱스면 OOV 단어를 추가\n",
    "            sentence.extend([vocabulary[OOV_INDEX]])\n",
    "            \n",
    "        # 빈칸 추가\n",
    "        sentence += ' '\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = BatchSequence(x_encoder, x_decoder, y_decoder)\n",
    "len(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3 박 4 일 놀 러 가 고 싶 다 <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_index_to_text(x_encoder[2], index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<START> 여행 은 언제나 좋 죠 <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_index_to_text(x_decoder[2], index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch : 1\n",
      "{'loss': [3.541323652733927, 1.7396523447140404, 1.5714846991974374, 1.4982836155787758, 1.4432028350622759, 1.3971302444520204, 1.3577766729437786, 1.3256785247636877, 1.2963844459989797, 1.2681558054426443, 1.2417016489350277, 1.2144042823625647, 1.1869707444439763, 1.1610628243373788, 1.137887970260952, 1.1175076870814613, 1.0982828431803247, 1.0802074748536814, 1.062230689370114, 1.0465151110421056, 1.0305719278428866, 1.016612301054208, 1.003957754243975, 0.991715435748515, 0.9806103486081829, 0.9690482415582823, 0.9590207584526228, 0.9491946995258331, 0.9395164244848749, 0.9312361673168514, 0.9220827098773874, 0.9140815676554389, 0.9057094383498897, 0.8986951592175857, 0.8908465316762095, 0.8837911920702975, 0.876490091500075, 0.8698211636232294, 0.8630648786606996, 0.8568725475798482, 0.8500056318614794, 0.8433882645938707, 0.837526754192684, 0.8317627602297327, 0.8256434565005095, 0.8199616657650989, 0.8145555529905402, 0.809163717472035, 0.8030636790006057, 0.7981260036644728, 0.7932418928198193, 0.7884622358757517, 0.7829411859097688, 0.7781270936779354, 0.7735233501247738, 0.7694384637086288, 0.7643280087605767, 0.759664746730224, 0.7555007241342379, 0.7512603084678235, 0.7466582092254058, 0.7428728840921236, 0.7382760209881741, 0.7340855268032654, 0.7305912064469379, 0.7267940102711968, 0.7220728539902231, 0.7188073765972386, 0.7149431219567424, 0.710872994168945, 0.7074714056823564, 0.7041057665710864, 0.7002813531004864, 0.6970247956721679, 0.693299758369508, 0.6898655972403028, 0.6867636403311854, 0.6834888416139976, 0.68049959903178, 0.6774146229676579, 0.6744063198566437, 0.6712896749377251, 0.6686049631756285, 0.6654910441973935, 0.6629393826360288, 0.6597287353614102, 0.6566754025609597, 0.6543862175034441, 0.6513030354095541, 0.6488969954459564, 0.646358026434546, 0.6435913054839425, 0.640382821145265, 0.6389040004300035, 0.6360025856158008, 0.6336878897703212, 0.6311193526439045, 0.6289085237228352, 0.6263201638408329, 0.6241590915166814], 'accuracy': [0.7143427, 0.72194576, 0.73302764, 0.7556301, 0.76120925, 0.7649768, 0.7696643, 0.7762087, 0.7794101, 0.78301346, 0.7885077, 0.7939255, 0.7976732, 0.8015285, 0.8051517, 0.80751246, 0.81007135, 0.81265, 0.81462294, 0.81638646, 0.8181584, 0.8194633, 0.82084185, 0.82216656, 0.82317144, 0.8243433, 0.82554346, 0.8265908, 0.82749945, 0.8278476, 0.82896566, 0.82993376, 0.83094144, 0.83141416, 0.8323681, 0.83302194, 0.8340127, 0.83494115, 0.83545065, 0.83626586, 0.83670175, 0.83768964, 0.83845955, 0.83921534, 0.84003055, 0.84030795, 0.84101, 0.8417488, 0.8427168, 0.84332544, 0.8441491, 0.8443614, 0.8450577, 0.8456267, 0.8460711, 0.8468665, 0.8475317, 0.848313, 0.8485224, 0.8492244, 0.85027456, 0.85067654, 0.85125965, 0.85204655, 0.85239756, 0.85275704, 0.85371095, 0.8540704, 0.85457146, 0.8549989, 0.855432, 0.85597825, 0.85673684, 0.8570794, 0.8580814, 0.85844654, 0.85885984, 0.8596411, 0.85984486, 0.8604818, 0.86076486, 0.8612121, 0.86186314, 0.8621632, 0.86284536, 0.86340296, 0.8642408, 0.86411065, 0.8646286, 0.8649485, 0.8655542, 0.86612034, 0.8665563, 0.86658174, 0.8671507, 0.86776775, 0.86796874, 0.8685632, 0.8691746, 0.86929065]}\n",
      "accuracy : 0.86929065\n",
      "loss : 0.6241590915166814\n",
      "잘 은 항상 좋 죠 \n",
      "\n",
      "Total Epoch : 2\n",
      "{'loss': [0.6212445423007011, 0.6187132383170335, 0.6166984416220499, 0.6145076120029325, 0.612174798289071, 0.6105625652100729, 0.6080417425736137, 0.6060394612343415, 0.6031416755007661, 0.6009882993024328, 0.599234694695991, 0.5976773173264835, 0.5948044492498689, 0.5932190055432527, 0.5911038398094799, 0.5894376978926037, 0.5873751387647961, 0.5852631035706272, 0.5824135460931322, 0.5808921394788701, 0.578911333952261, 0.5773475370977236, 0.5755630070748536, 0.5733492218929789, 0.5714085568552432, 0.5694970158131226, 0.5674071580819462, 0.565622738522032, 0.5639093790365302, 0.561394973617533, 0.5595551926804625, 0.5577683027671732, 0.5550167923388274, 0.5541234000221543, 0.5523825585842133, 0.5500609178258025, 0.5482083160592162, 0.5467097260381865, 0.5446081598815711, 0.5419559744389161, 0.5411108482791029, 0.5389565669971964, 0.5373471735612206, 0.5357163908041042, 0.5342332782304805, 0.5322173108225283, 0.5302505127113798, 0.5290812056349672, 0.5269113494002301, 0.5249935041951097, 0.5231450268107912, 0.5216461300202038, 0.5198950650899307, 0.517634741961956, 0.5162504027071206, 0.5149082227245622, 0.5130101144313812, 0.5123584144141363, 0.5097153922138007, 0.5079971101620923, 0.5068208406800809, 0.5053884468000868, 0.5038733148704404, 0.5014803105074427, 0.5007743113066839, 0.49911446545435034, 0.49705433813126193, 0.49544404706229334, 0.4938126144849736, 0.4923363784733026, 0.4905235718773759, 0.48892030119895935, 0.4882931955482649, 0.48596158170181775, 0.48357192264950793, 0.4836820859623992, 0.48054143203341443, 0.47989151918369793, 0.4787281822899114, 0.47720342194256576, 0.47495429606541345, 0.4734339056455571, 0.47145342243754346, 0.4709766808411349, 0.4689433568197748, 0.4684524034028468, 0.46610313815915067, 0.4645063064020613, 0.46392224372729013, 0.4616654858641002, 0.46039408963659534, 0.45906482701716217, 0.4575762301683426, 0.45646128285190335, 0.45430879035721655, 0.45340335012777994, 0.45152378438607504, 0.4503748180425685, 0.4498818817993869, 0.4473264985110449], 'accuracy': [0.86998415, 0.870321, 0.870805, 0.8714136, 0.87125224, 0.871657, 0.8723307, 0.8725345, 0.8733526, 0.8733979, 0.8737857, 0.87393284, 0.87474805, 0.875184, 0.8754614, 0.8756907, 0.8756454, 0.87627095, 0.8770041, 0.87740886, 0.87736076, 0.8777089, 0.8782665, 0.87842506, 0.87881285, 0.879362, 0.8795403, 0.8799791, 0.8802678, 0.8804687, 0.8810235, 0.88157266, 0.88195485, 0.8820397, 0.88241905, 0.8830701, 0.883707, 0.8833107, 0.8839957, 0.8846892, 0.88474584, 0.8849298, 0.88522136, 0.8861272, 0.8862659, 0.8863451, 0.88682634, 0.8871773, 0.88727355, 0.8878425, 0.88802934, 0.8881397, 0.88923514, 0.88962865, 0.88958335, 0.88986075, 0.8903335, 0.890257, 0.8908458, 0.8912534, 0.8915761, 0.8916072, 0.89192426, 0.8923092, 0.8923687, 0.8927876, 0.89323485, 0.89335936, 0.8940302, 0.8941718, 0.8945426, 0.8944973, 0.8946813, 0.8952785, 0.89602864, 0.89553326, 0.89667684, 0.8964221, 0.89639664, 0.8967448, 0.89712125, 0.8973279, 0.8979648, 0.8976251, 0.8983809, 0.89822805, 0.8986838, 0.89938575, 0.89907724, 0.89923006, 0.9000198, 0.89972544, 0.8997226, 0.9005293, 0.9005491, 0.9010105, 0.9012398, 0.90167004, 0.9017295, 0.9025249]}\n",
      "accuracy : 0.9025249\n",
      "loss : 0.4473264985110449\n",
      "저 은 항상 좋 죠 \n",
      "\n",
      "Total Epoch : 3\n",
      "{'loss': [0.4459033209992492, 0.444378604059634, 0.4430217762356219, 0.4413652309904928, 0.44008698670760443, 0.43793097656706104, 0.43687081596125726, 0.43707253816335095, 0.43495803747488104, 0.4338432644372401, 0.43115651931451715, 0.4313139678991359, 0.42922216880580655, 0.4274787148055823, 0.4257695985876996, 0.42614154388075287, 0.42428800690433255, 0.42218535393476486, 0.4215833599800649, 0.4209389207155808, 0.4189315425313037, 0.4182218439553095, 0.41646378066228784, 0.41538168425145355, 0.41376226168611774, 0.4127975277926611, 0.41165521838094876, 0.41038064237522043, 0.4097425539208495, 0.40769642235144327, 0.40617454861817154, 0.4056133031845093, 0.4044113311430682, 0.40257511417502945, 0.40185198803310807, 0.40114971898172214, 0.40015119621935097, 0.39833471000842424, 0.39630238630849385, 0.3961206810629886, 0.3942721520105134, 0.3927379597140395, 0.39305014211846434, 0.39061920069482015, 0.3899315124296624, 0.38870779441102693, 0.38852491991027543, 0.3870902851871822, 0.3852932352734649, 0.3839907795190811, 0.3833429712964141, 0.3821319432362266, 0.3804818105114543, 0.3791035818664924, 0.37825899004288344, 0.3789575002763582, 0.3764987630040749, 0.37604104387371434, 0.37381199957883876, 0.37247843052382057, 0.3718771821130877, 0.36991344133149023, 0.369214311728011, 0.369470648791479, 0.36732303059619403, 0.36647151637336484, 0.3645060225997282, 0.3648287830469401, 0.36368290529302927, 0.3620824084981628, 0.3609080473365991, 0.3616524779278299, 0.35856911329471547, 0.35793714225292206, 0.35696149082935374, 0.3546318410207396, 0.3546299600730772, 0.35320744857839914, 0.3517840784852919, 0.35170684430910193, 0.3490662830679313, 0.3480218101454818, 0.347698372181343, 0.34592698660233745, 0.3451904405070388, 0.34466451260706654, 0.34295107618622156, 0.3431077787409658, 0.3417567205817803, 0.34025940736350807, 0.33866269190026366, 0.3375439844701601, 0.33742405814321147, 0.336912403450064, 0.3343445749386497, 0.3333986540851386, 0.3332303619903067, 0.3318836319705714, 0.3307746916037539, 0.3297927041092645], 'accuracy': [0.90225315, 0.90260136, 0.9031788, 0.90320426, 0.9033656, 0.9040025, 0.9041072, 0.90359205, 0.9044667, 0.9044582, 0.9048347, 0.9048177, 0.90527064, 0.9058367, 0.90596694, 0.9055593, 0.9062302, 0.9067227, 0.9068416, 0.9067454, 0.90747, 0.9074077, 0.9076087, 0.90777004, 0.9087155, 0.90795684, 0.9082229, 0.9087183, 0.9087268, 0.9090523, 0.9097996, 0.9093467, 0.9098392, 0.91018736, 0.910561, 0.91031194, 0.91043365, 0.91073936, 0.91092336, 0.9111158, 0.91172725, 0.9120471, 0.91160554, 0.9120726, 0.91228485, 0.9127406, 0.9128963, 0.9129416, 0.9132162, 0.9136436, 0.9136945, 0.91363794, 0.9141418, 0.91410214, 0.91437954, 0.9142153, 0.914906, 0.9148494, 0.91553444, 0.9156363, 0.9156958, 0.9161713, 0.91616845, 0.9160156, 0.91661286, 0.91616, 0.91686195, 0.9171281, 0.91709125, 0.9174706, 0.9174281, 0.9175272, 0.918048, 0.91815275, 0.91841596, 0.918951, 0.9185377, 0.9190246, 0.91901326, 0.918934, 0.9197294, 0.91970676, 0.92007756, 0.9206465, 0.9205616, 0.9204144, 0.92091256, 0.9209182, 0.9212183, 0.92135984, 0.92157495, 0.9218297, 0.9219797, 0.92188066, 0.9224751, 0.92289686, 0.9228459, 0.9229818, 0.92298746, 0.92337805]}\n",
      "accuracy : 0.92337805\n",
      "loss : 0.3297927041092645\n",
      "피할 은 언제나 좋 죠 \n",
      "\n",
      "Total Epoch : 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [0.32858292742267897, 0.32768267441702925, 0.3267730077647645, 0.324639140587786, 0.32424516133640124, 0.3235150361838548, 0.3218792027429394, 0.32058547939295357, 0.3197461637141912, 0.31759721494239307, 0.3172932103600191, 0.3160459157889304, 0.31523717956050584, 0.3146335519850254, 0.31339028817803966, 0.3120879885619101, 0.3103091149874356, 0.30942148342728615, 0.30784704746759456, 0.30781503118898557, 0.3064639940857887, 0.30422996373280237, 0.30389785264497216, 0.30268416165009787, 0.3016317323174166, 0.30005157253016596, 0.3001830399684284, 0.2989211573225001, 0.2971497529550739, 0.29623557530019595, 0.2943846508860588, 0.2937918253566908, 0.2924216378318227, 0.29201519586469815, 0.2906162210780641, 0.2887384063847687, 0.2878670177381972, 0.28710286286862, 0.28656708724472835, 0.28452472307759785, 0.2834010001109994, 0.2825942251669324, 0.2817802983133689, 0.2794242081758769, 0.27897240545438684, 0.27802760639916296, 0.27660961060420325, 0.27613396975009336, 0.27424211022646533, 0.2743648502813733, 0.273781833279392, 0.27218913222136704, 0.2703226135800714, 0.2688268641738788, 0.2693038307454275, 0.26753677751706995, 0.26602341639606847, 0.26507866949490877, 0.26389218316130014, 0.26399633628518687, 0.2625763613892638, 0.2617022322247858, 0.2598065578419229, 0.2599415651158146, 0.25827730657613795, 0.25790218442030577, 0.25675149407723674, 0.2553367886854255, 0.2551106896413409, 0.2541189305484295, 0.252307726799146, 0.2516103767506454, 0.2504416299254998, 0.2498330125666183, 0.24785359381981517, 0.24818456901804262, 0.24652994019181831, 0.2459048129942106, 0.2454176182332246, 0.24474188175214373, 0.24350389481886572, 0.24282614255081053, 0.24212153474597828, 0.24067939745019312, 0.23906609683257082, 0.23932432828714018, 0.2387785407026177, 0.23684790973430095, 0.2350758152163547, 0.23502039472046105, 0.23469319567084312, 0.23420457849683968, 0.23347595010114752, 0.23247282644328865, 0.23131648329613003, 0.22992553659107373, 0.2290652433167333, 0.22879737000102582, 0.22821716154399124, 0.226868841878098], 'accuracy': [0.923463, 0.92382246, 0.92385924, 0.924499, 0.92460936, 0.9243829, 0.9251274, 0.924983, 0.9254076, 0.92585766, 0.9259086, 0.92622566, 0.92624265, 0.9260502, 0.9268852, 0.92672104, 0.9270324, 0.9275419, 0.92785895, 0.9275702, 0.9279948, 0.92827505, 0.92852414, 0.92860335, 0.9287364, 0.9292997, 0.9291157, 0.92929685, 0.92969596, 0.9298121, 0.93008095, 0.93054235, 0.93054515, 0.9303923, 0.93054515, 0.9317369, 0.9315274, 0.931584, 0.9319803, 0.9320567, 0.9325889, 0.93262285, 0.9326087, 0.9332767, 0.93326825, 0.933557, 0.9338598, 0.9338825, 0.9345052, 0.934024, 0.9342278, 0.9341995, 0.9349723, 0.9353572, 0.93485904, 0.93585825, 0.93588936, 0.935963, 0.93624884, 0.9364951, 0.9365291, 0.9368801, 0.93718016, 0.9368546, 0.93755376, 0.93765, 0.93784535, 0.9381935, 0.93815386, 0.93854165, 0.93904835, 0.9388389, 0.9391361, 0.9393682, 0.93938804, 0.93980694, 0.9400136, 0.9402372, 0.9403957, 0.9403221, 0.94057685, 0.9405514, 0.9410411, 0.9413638, 0.94143456, 0.94140345, 0.94133264, 0.94188744, 0.94253564, 0.9424451, 0.9425498, 0.94268566, 0.94292915, 0.94325745, 0.94332826, 0.9439821, 0.9439623, 0.9437132, 0.9439396, 0.9440501]}\n",
      "accuracy : 0.9440501\n",
      "loss : 0.226868841878098\n",
      "여행 은 언제나 좋 죠 \n",
      "\n",
      "Total Epoch : 5\n",
      "{'loss': [0.22624179153986598, 0.22558483076484306, 0.2240904213293739, 0.22410295806501224, 0.22325387532296387, 0.22214344508298065, 0.2218131460249424, 0.22083008321730987, 0.21897756389301756, 0.21913485642036665, 0.21866204289962415, 0.2176467930979055, 0.21731524137051209, 0.21534180948915688, 0.21507498468069927, 0.21382356985755588, 0.21261835956703062, 0.21239625087574773, 0.21173574111383894, 0.21210794588146004, 0.21000353554668633, 0.2099391559705786, 0.21041089474507, 0.20824701913996882, 0.2066131716027208, 0.2067443932528081, 0.2063007154704436, 0.20568322317431803, 0.20482926961520445, 0.20347212578939355, 0.20326636496769346, 0.20381356962025166, 0.20222752346940662, 0.20226959179600942, 0.20109118391638217, 0.19957280669199384, 0.1991195811525635, 0.19880839550624724, 0.19864086077912993, 0.19751918299690538, 0.19844031528286313, 0.19620551792499813, 0.1961590589388557, 0.19436769236041151, 0.19504579278114048, 0.19343102690966232, 0.19299181502150453, 0.19263978083820446, 0.19132762021668578, 0.19134217565474304, 0.19154610776382944, 0.18891997230441673, 0.18918569070165572, 0.1895447945141274, 0.18716272760344588, 0.18766606607190942, 0.18578643027855002, 0.18550605748010718, 0.18505603658116382, 0.18458970700917038, 0.1844215004340462, 0.18417524416809497, 0.18279629346469176, 0.18250298070842805, 0.18213585132490034, 0.1815988127466129, 0.1805740839437298, 0.18031731571840204, 0.1799229374560325, 0.178111821817963, 0.17912324841903604, 0.17797624408874824, 0.17604671054236268, 0.1763496837052314, 0.17623170287064885, 0.17524319628010626, 0.17468398445002412, 0.17437861705927746, 0.1744079849804225, 0.17397845325910527, 0.1731576839380938, 0.17227054265854153, 0.17140383581104485, 0.17149507084294505, 0.17150550716273164, 0.1697683250936477, 0.16821796260774136, 0.16877737746614477, 0.16790955578503403, 0.16719591860537944, 0.16653577578456505, 0.1667347432964522, 0.16567515735716923, 0.16580031520646552, 0.16492368288986062, 0.16468475856210874, 0.16412642614349074, 0.16371831289775993, 0.16361156316555064, 0.1627215415077365], 'accuracy': [0.94457936, 0.94466996, 0.944636, 0.94502944, 0.94530404, 0.9452078, 0.9454371, 0.94579655, 0.9461277, 0.94619846, 0.945805, 0.9464023, 0.94634, 0.9468042, 0.9471807, 0.94735336, 0.94803554, 0.9479846, 0.94814026, 0.9477327, 0.94831294, 0.94832426, 0.9478063, 0.94874036, 0.94834125, 0.9484913, 0.9489838, 0.9490461, 0.94926685, 0.94934046, 0.9497679, 0.94954145, 0.9494254, 0.9498755, 0.9501019, 0.95040196, 0.95051515, 0.95071614, 0.95094544, 0.95091146, 0.9510247, 0.9511634, 0.95100486, 0.9515625, 0.95159364, 0.95188516, 0.95186824, 0.9523664, 0.95251924, 0.9522192, 0.95230126, 0.9528023, 0.9526268, 0.95262396, 0.95337975, 0.9532835, 0.9533684, 0.9538921, 0.9539204, 0.9540308, 0.95377886, 0.95384115, 0.95432234, 0.95442426, 0.9548941, 0.95435065, 0.9550385, 0.9547271, 0.9547583, 0.9553725, 0.95503, 0.9552791, 0.95630944, 0.95580274, 0.9557886, 0.95612544, 0.9561198, 0.9561028, 0.9560462, 0.956301, 0.9562783, 0.9566406, 0.95677084, 0.95686144, 0.95668024, 0.9576228, 0.95780116, 0.9575153, 0.95767945, 0.958305, 0.9584069, 0.9579512, 0.9583192, 0.9582824, 0.9584579, 0.95826256, 0.9587523, 0.9586532, 0.9588004, 0.95902115]}\n",
      "accuracy : 0.95902115\n",
      "loss : 0.1627215415077365\n",
      "여행 은 언제나 좋 죠 \n",
      "\n",
      "Total Epoch : 6\n",
      "{'loss': [0.16132577014682087, 0.1615095461678246, 0.16090676147976649, 0.16079259550441866, 0.15936201956609022, 0.15909083479124567, 0.15917095873990786, 0.15801664393233217, 0.15776304981630782, 0.15818859183269998, 0.15669004687958438, 0.15711823616014875, 0.15559274885479524, 0.15509525313973427, 0.15503503212138361, 0.15480275812518338, 0.1539841172001932, 0.15404674281244693, 0.1535860090718969, 0.15299824090755504, 0.15202781351526146, 0.1528070807133032, 0.1508686322637874, 0.15159604540499655, 0.15069754887372255, 0.15015284810215235, 0.15018574913720723, 0.14966043129401363, 0.1484172306145015, 0.1484773043624085, 0.1481137784278911, 0.14789951977360508, 0.14737038091634927, 0.14746181517029586, 0.1461478993701546, 0.14569519974453293, 0.1464646929146155, 0.14455253918371772, 0.14512738735293565, 0.1445923882496098, 0.14315838354599217, 0.1432409388539584, 0.1427989665256894, 0.14212860733918523, 0.1421833107166964, 0.1411400166461649, 0.14111773827639612, 0.1406280861195663, 0.14019352744292954, 0.1390922580562208, 0.14019261713585127, 0.13949571436514024, 0.13829371531534454, 0.13864893320461977, 0.1374660104351199, 0.13789007812738419, 0.13612129267953013, 0.13623633138511493, 0.13718880312112364, 0.13554337349436854, 0.13582313732932444, 0.13585368807063156, 0.13557952221321024, 0.13448544401351525, 0.13434502571497275, 0.1338934948991822, 0.13322761144651019, 0.13286168221384287, 0.13287303260649028, 0.13223735558921876, 0.13209849183002245, 0.13141939793106006, 0.13057442819294723, 0.1304572806896075, 0.1299203743186334, 0.12973816656386075, 0.13062629812275586, 0.12907279181577588, 0.1292320979030236, 0.12860389110510764, 0.12784962655733462, 0.12664001560567514, 0.12741054307021524, 0.12652330161274775, 0.12643037266705348, 0.1268294140615541, 0.12554155030976172, 0.12462786617486374, 0.12458798297397468, 0.12520517963592126, 0.12404446899081054, 0.12368871878994547, 0.12383941565032887, 0.12300093937665224, 0.12304795432187941, 0.1228526589744117, 0.12252512489162061, 0.12194760062772295, 0.12216246091639218, 0.12191098503282537], 'accuracy': [0.95911175, 0.9594741, 0.95911175, 0.95928156, 0.9596892, 0.9598449, 0.9597033, 0.96003455, 0.9603204, 0.96011376, 0.9603544, 0.96040535, 0.9606488, 0.96102524, 0.96091485, 0.9607394, 0.9610281, 0.9611838, 0.9614017, 0.96119225, 0.96151495, 0.9611526, 0.96184045, 0.9613168, 0.96183765, 0.96182066, 0.9617414, 0.9622792, 0.9622028, 0.9622198, 0.9624943, 0.9622651, 0.9626302, 0.96243775, 0.96273494, 0.96282554, 0.96246606, 0.96325576, 0.9629076, 0.96334916, 0.9637426, 0.96357566, 0.9637681, 0.9638474, 0.9637313, 0.9640087, 0.9642012, 0.9641587, 0.9641757, 0.9645154, 0.96425784, 0.9646824, 0.9648947, 0.9646824, 0.9647192, 0.96461445, 0.9650957, 0.96529096, 0.9649683, 0.96514946, 0.9651721, 0.9650419, 0.9655712, 0.96574104, 0.9658401, 0.9653731, 0.9659449, 0.9660581, 0.9658939, 0.9661062, 0.9662902, 0.966361, 0.9666327, 0.9668875, 0.9667629, 0.9666412, 0.9666355, 0.9669328, 0.9668252, 0.9670771, 0.96713656, 0.9676432, 0.96729225, 0.9674366, 0.9676772, 0.967564, 0.96784985, 0.9679999, 0.968198, 0.9677932, 0.9681754, 0.96829426, 0.96820086, 0.96834236, 0.9684132, 0.96877265, 0.96852356, 0.9685009, 0.9682858, 0.9688066]}\n",
      "accuracy : 0.9688066\n",
      "loss : 0.12191098503282537\n",
      "여행 은 언제나 좋 죠 \n",
      "\n",
      "Total Epoch : 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [0.12115713540950547, 0.12073951558736355, 0.12059599197591128, 0.1197612221636202, 0.12049703352639209, 0.11948253473509914, 0.12042666599154472, 0.11993563312875188, 0.11877088167745134, 0.11891821314297292, 0.11833197846198859, 0.11735964380204678, 0.11665182589026897, 0.1173143510747215, 0.11662360612788926, 0.1160451899809034, 0.11666667967548837, 0.11606043670326471, 0.11515027086209992, 0.1150719029828906, 0.11576448036762683, 0.11448533136559569, 0.11464910638397155, 0.11469649539693542, 0.11367967684307824, 0.11388191293277171, 0.11325336219337971, 0.1125578758833201, 0.11210873646094748, 0.1126664207600381, 0.11252371221780777, 0.11176847010526968, 0.11143224245018285, 0.11060210329520961, 0.11065019825068505, 0.11068195149140514, 0.11060572958186916, 0.11047833627494781, 0.10990505317307037, 0.10902916968030774, 0.10988745595449986, 0.10892453862597114, 0.10890168441540521, 0.10837333264720181, 0.10864243036864893, 0.10784609636048907, 0.10832709426303273, 0.10780447235573894, 0.10686570628667655, 0.10727603494635095, 0.10563650970225749, 0.10641476694170547, 0.10591253881221233, 0.10661940414296545, 0.10508411280486894, 0.10605022064207689, 0.10452296289250902, 0.10470971412470807, 0.10441880373527175, 0.10385644565457883, 0.10414717305937539, 0.10353875237152628, 0.10378477297237386, 0.10323463165727646, 0.10328987852224837, 0.10252451317627793, 0.10205970607374025, 0.10133188914345659, 0.10164835741338522, 0.10051073680591324, 0.1007274966686964, 0.1009419380241762, 0.10076500702163448, 0.10126020178279799, 0.09969569495676653, 0.10013670960198277, 0.0997158682135784, 0.09929483663290739, 0.09946868669889543, 0.09874487575143576, 0.09859048828239673, 0.09829398090748684, 0.09910238817658114, 0.09729723361037347, 0.09816680063048135, 0.09774120982088473, 0.09744614656285747, 0.0969726516176825, 0.09645425584977088, 0.09630886906676966, 0.09602823442496035, 0.0974120627278867, 0.09602391816999602, 0.0958484674844405, 0.09592586030940646, 0.09564331562622734, 0.09473460694046124, 0.09505150459058907, 0.09329575664647248, 0.09452904307323953], 'accuracy': [0.9687359, 0.9691208, 0.9690642, 0.96940106, 0.96893114, 0.9693189, 0.969084, 0.9691293, 0.9694152, 0.96903306, 0.9695369, 0.96990204, 0.96985394, 0.9696728, 0.969919, 0.9702927, 0.9699672, 0.969987, 0.9701257, 0.9705078, 0.9698285, 0.9704229, 0.9703804, 0.97034645, 0.9706182, 0.970488, 0.9706805, 0.9709862, 0.9710626, 0.9708135, 0.97102016, 0.97130036, 0.9714646, 0.97154945, 0.9713117, 0.9713542, 0.97126925, 0.9714447, 0.9713372, 0.9719061, 0.97157496, 0.9719061, 0.97188634, 0.97211844, 0.97187215, 0.9720675, 0.9717816, 0.9719967, 0.9724638, 0.97216374, 0.9726902, 0.9723675, 0.97248644, 0.97242695, 0.97254866, 0.97225994, 0.9730978, 0.9729393, 0.97295344, 0.9731261, 0.9730356, 0.973078, 0.972928, 0.97306955, 0.97299594, 0.9732365, 0.9734828, 0.97383946, 0.973463, 0.9736951, 0.9737574, 0.97384226, 0.97388756, 0.9737913, 0.97378284, 0.97407436, 0.97404045, 0.97422725, 0.97385645, 0.97421306, 0.9745867, 0.97434044, 0.9740008, 0.9747113, 0.9742187, 0.9746801, 0.9744226, 0.9745414, 0.97494906, 0.9748641, 0.97494054, 0.9745018, 0.9748641, 0.9750538, 0.97499436, 0.9753283, 0.9751868, 0.97527456, 0.97565955, 0.9752123]}\n",
      "accuracy : 0.9752123\n",
      "loss : 0.09452904307323953\n",
      "여행 은 언제나 좋 죠 \n",
      "\n",
      "Total Epoch : 8\n",
      "{'loss': [0.0934852476677169, 0.09398309372203506, 0.09345902053072401, 0.093461603310931, 0.09383183276361745, 0.09286497197235408, 0.09289039410245807, 0.09357390615522214, 0.09234542537318624, 0.09203721426994256, 0.09258817328626047, 0.09266405151752026, 0.0919422886091406, 0.09230881988111397, 0.0908283184322974, 0.09021382930729052, 0.0904412955734069, 0.09121988729938217, 0.0902454968460876, 0.08938312297686934, 0.08994341226618575, 0.08963126089909802, 0.08924436968062884, 0.08897773447729972, 0.08890299851317769, 0.0890389249417121, 0.0889851094385528, 0.08841252533475989, 0.0878707873634994, 0.08749137411865851, 0.08814491819509346, 0.08807465911883375, 0.08755079938260757, 0.08640578775869115, 0.08771603026062898, 0.08676751885239196, 0.08664678282144925, 0.08596340736941151, 0.08523293133096202, 0.08580116777802291, 0.08577500824290125, 0.08497593373708102, 0.08565270716486417, 0.08471889202685459, 0.08495760107736873, 0.08531735562112021, 0.08497339513395792, 0.08354327404547644, 0.08461410299186474, 0.08373844475768831, 0.08361370602379674, 0.08399254225356423, 0.08397352825040402, 0.08332045613180684, 0.0818913861218354, 0.08298384147169797, 0.08297230070456862, 0.08246220136061311, 0.0825642153782689, 0.08175201092482261, 0.08183197855301526, 0.08146048673307119, 0.08060056276862389, 0.0816810563125688, 0.08151667135889115, 0.08089507458245625, 0.08018370926299173, 0.08073328251181089, 0.08051663398256768, 0.08031787893370441, 0.08063431272445165, 0.0802978888032553, 0.08018199720865359, 0.07824978406500557, 0.07986467313426344, 0.07958833695105884, 0.07940429756822794, 0.07900742601360315, 0.07976287240972338, 0.07832194164233364, 0.0778299466220905, 0.07766734306579051, 0.07849068364695362, 0.07872622836705136, 0.07838210372416221, 0.07753502678773973, 0.07651083811145762, 0.077505046400525, 0.07684691971086938, 0.07686201444543574, 0.07741425020377272, 0.07687219631169802, 0.07672000267421422, 0.07563924202290566, 0.07688556433371875, 0.0758165393591575, 0.075990896469549, 0.07580822319521205, 0.07518341475287857, 0.07449747622013092], 'accuracy': [0.9755293, 0.9753736, 0.9756963, 0.9754869, 0.9756256, 0.97577274, 0.9754076, 0.975317, 0.975835, 0.9759313, 0.9757897, 0.9754897, 0.976237, 0.97577274, 0.9761351, 0.9765342, 0.9763134, 0.97612095, 0.9764804, 0.9766418, 0.976404, 0.9765342, 0.9768031, 0.9767946, 0.9767323, 0.97666156, 0.97665024, 0.9771032, 0.9771173, 0.977072, 0.9768229, 0.97669554, 0.9772249, 0.97746545, 0.9768569, 0.9773466, 0.9773806, 0.97753626, 0.9778929, 0.9776127, 0.97744846, 0.97787875, 0.9775164, 0.97768056, 0.97781926, 0.97753626, 0.9774683, 0.97826934, 0.97746545, 0.9781165, 0.97794104, 0.9777315, 0.9777627, 0.97818726, 0.9785864, 0.97817594, 0.9781986, 0.9782128, 0.9781675, 0.97858924, 0.97847885, 0.97846466, 0.9785864, 0.9781675, 0.97864586, 0.97891194, 0.9789997, 0.97885245, 0.9788157, 0.9788723, 0.978493, 0.9787166, 0.97878456, 0.97929686, 0.9788893, 0.9790393, 0.9789515, 0.97911286, 0.97892606, 0.97928554, 0.97942144, 0.9794384, 0.9792148, 0.9790478, 0.9792912, 0.9794186, 0.9799111, 0.97938746, 0.97961956, 0.9795856, 0.97917235, 0.9796309, 0.97960544, 0.9801036, 0.97969884, 0.9799224, 0.97974694, 0.9799253, 0.980197, 0.9802168]}\n",
      "accuracy : 0.9802168\n",
      "loss : 0.07449747622013092\n",
      "여행 은 언제나 좋 죠 \n",
      "\n",
      "Total Epoch : 9\n",
      "{'loss': [0.07478912944055122, 0.07476888364180923, 0.0750321040496878, 0.07493228420777165, 0.07489648352012686, 0.07497361091815907, 0.07352725471086476, 0.07399583356859891, 0.07432228581898886, 0.07372998185050876, 0.07334303959151325, 0.07236042254320953, 0.07334679450191882, 0.07249426064283951, 0.07288173588154756, 0.07233806644849804, 0.07209986628478636, 0.07295293747649892, 0.07245902107704592, 0.07208208268022408, 0.07181303486551928, 0.0716918503217723, 0.07197596736090339, 0.07154337264111509, 0.07047337761310779, 0.07072862721574695, 0.07100215236372921, 0.07082489853643853, 0.07116954421381587, 0.07052113826427123, 0.07099040182631301, 0.07000981631648281, 0.06976631893168973, 0.07034655030735809, 0.07015487247997004, 0.0696955704656632, 0.06976954191518218, 0.06974599492209761, 0.06977947941049933, 0.06957517853816567, 0.06961671639557766, 0.06934313431544148, 0.06814579088645785, 0.06874114370135509, 0.0687642044833173, 0.06819877682415687, 0.0680336717961599, 0.06858694219313886, 0.06791058709116085, 0.06796772219240665, 0.06789411535567563, 0.06785741399811662, 0.0672267401267005, 0.06790612107547729, 0.06737604648198771, 0.06759773079386872, 0.06760636206878268, 0.06679606951935135, 0.06713892918323046, 0.06814840124191149, 0.06634752306601276, 0.06625627538027323, 0.06585965605209702, 0.06563536225534651, 0.0654312502997725, 0.0666448657967798, 0.06610344694522412, 0.0662976849378775, 0.06563995298727052, 0.06586076811198956, 0.06546166292189257, 0.06575122397676435, 0.06556376168990265, 0.06623638951746018, 0.06407254023234481, 0.06502628411449816, 0.06423376439868109, 0.06478865821238446, 0.06463702796431987, 0.06479331057356752, 0.06414686681945687, 0.06372407977671726, 0.06313610225713448, 0.06343817312027449, 0.06346431169051515, 0.06368444659544722, 0.06333250210256032, 0.06339016692388964, 0.06383400373970685, 0.06298800019304389, 0.06271374424028656, 0.06301799724283426, 0.06290890337170466, 0.06305466452613473, 0.06273034205862685, 0.06306586787104607, 0.06213260954245925, 0.06147487907994376, 0.06251239234014698, 0.06256177999160212], 'accuracy': [0.9801234, 0.98040646, 0.98008096, 0.98006964, 0.9799989, 0.98018855, 0.9804178, 0.980364, 0.98010075, 0.9805027, 0.9806442, 0.9807518, 0.980432, 0.9807094, 0.98055935, 0.9807688, 0.9810349, 0.9805395, 0.98072916, 0.9805961, 0.9806414, 0.9808509, 0.9808282, 0.98088765, 0.9813066, 0.9811651, 0.98105466, 0.9814, 0.9810575, 0.9813349, 0.9813349, 0.981417, 0.9813434, 0.9812642, 0.981233, 0.9813859, 0.9811509, 0.9813717, 0.9814057, 0.9813859, 0.9815104, 0.9815868, 0.98188686, 0.98162365, 0.9817255, 0.9818812, 0.9818699, 0.9816803, 0.9816774, 0.98183876, 0.9819124, 0.9818133, 0.98204255, 0.9819407, 0.98185295, 0.9817708, 0.9817963, 0.98225206, 0.9818614, 0.98182744, 0.98224354, 0.9823398, 0.9824502, 0.9826087, 0.9824672, 0.9820765, 0.9824785, 0.98234546, 0.9826653, 0.9821473, 0.9823143, 0.98222375, 0.9824247, 0.9821728, 0.9827927, 0.98270774, 0.98279834, 0.9823568, 0.9827219, 0.98271906, 0.9825379, 0.9827927, 0.9831918, 0.9830701, 0.9829993, 0.9828153, 0.9828521, 0.9828691, 0.9829993, 0.98293424, 0.9829399, 0.98296255, 0.98310405, 0.98299366, 0.9832456, 0.98316634, 0.9835145, 0.9833418, 0.98330504, 0.98316634]}\n",
      "accuracy : 0.98316634\n",
      "loss : 0.06256177999160212\n",
      "여행 은 언제나 좋 죠 \n",
      "\n",
      "Total Epoch : 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [0.06225042796248327, 0.061126549252430384, 0.06184562012229277, 0.061540761274164135, 0.0609439079163839, 0.06078383973394723, 0.0610976036152114, 0.062079101983133864, 0.06142394645544498, 0.06086263809677051, 0.06053202433268661, 0.060675436995275646, 0.06106294634873453, 0.06061234740216447, 0.060260610746057784, 0.0606402861683265, 0.06044297575262254, 0.05936581342805015, 0.061008892244780843, 0.059935997867875776, 0.05967325089580339, 0.0598648385974862, 0.05938885261750092, 0.06006458063569406, 0.05933727704636429, 0.05921030658013795, 0.05872379741428987, 0.058581514122045555, 0.0590156185562196, 0.05887597352873696, 0.059007493779063225, 0.058276979212203754, 0.058818682856128915, 0.0583018916355365, 0.058050724434787815, 0.05811893304243036, 0.05794606410452853, 0.058842878626740494, 0.0586617774249095, 0.05768059140435704, 0.057500426748605525, 0.05761350490881697, 0.05742386906691219, 0.05676329625851434, 0.05735754960661997, 0.057534265767216035, 0.05797226356504404, 0.05720967750834382, 0.05689699769667957, 0.05659537938544932, 0.0566475676048709, 0.057459927295618086, 0.0560365882297249, 0.05686392374944104, 0.05627935717854163, 0.05630616866983473, 0.05650213146177323, 0.05610121899972791, 0.05602142447605729, 0.055843135483724916, 0.0560478459658992, 0.056439764261164746, 0.056173589951156275, 0.05593302946173302, 0.05658242211717626, 0.055998249981633344, 0.055096876240618854, 0.055599725015623415, 0.055133125957344535, 0.05517619538485356, 0.055169851610275066, 0.0554444134923751, 0.05449688544942309, 0.054253697354832424, 0.05456874343445119, 0.054793348292941635, 0.05539660606249843, 0.0537213232246754, 0.0546380539789148, 0.05441219508445457, 0.05484809873261205, 0.05444493856402519, 0.05409278812737245, 0.053982821677851935, 0.05474367787373131, 0.05374413036295901, 0.05462272852942671, 0.05367272567894796, 0.054026458746470184, 0.053480200125309435, 0.054030479640578444, 0.05237677875582291, 0.05285926690608587, 0.053358842628886516, 0.05300371908663731, 0.0522491198397525, 0.052253418237618775, 0.053201683381896306, 0.05311575071359782, 0.05277526860489794], 'accuracy': [0.98330504, 0.9834805, 0.98339844, 0.98361355, 0.98365605, 0.9836985, 0.9835909, 0.98322296, 0.9833843, 0.9833928, 0.9837919, 0.9835626, 0.9834182, 0.98355126, 0.9838542, 0.98353434, 0.9837891, 0.98399854, 0.98363054, 0.98387116, 0.9838315, 0.9839023, 0.9840042, 0.9835909, 0.98410326, 0.984041, 0.98425895, 0.98418534, 0.98396736, 0.98400134, 0.9838202, 0.9842533, 0.984024, 0.98419666, 0.9842476, 0.98416835, 0.9842957, 0.984341, 0.984024, 0.984208, 0.9845165, 0.9844967, 0.98453915, 0.98476, 0.9844373, 0.98428726, 0.9842844, 0.984576, 0.98436654, 0.98478544, 0.98472315, 0.9842986, 0.98490715, 0.9846383, 0.9847939, 0.98469204, 0.9844769, 0.9848987, 0.9847118, 0.984777, 0.98484486, 0.98472035, 0.98460144, 0.98470336, 0.9846524, 0.9848194, 0.9851279, 0.9848817, 0.9851053, 0.98493546, 0.98511094, 0.9850374, 0.9851902, 0.98530626, 0.9850402, 0.984927, 0.9850317, 0.9854393, 0.98513925, 0.98507136, 0.98509115, 0.9851251, 0.9852978, 0.98536, 0.9851421, 0.98534876, 0.9851251, 0.9853912, 0.98521566, 0.98548746, 0.9852185, 0.9858243, 0.9857337, 0.9853742, 0.98544496, 0.98571104, 0.9859205, 0.9853148, 0.9854195, 0.9855129]}\n",
      "accuracy : 0.9855129\n",
      "loss : 0.05277526860489794\n",
      "여행 은 언제나 좋 죠 \n",
      "\n",
      "CPU times: user 6h 59min 17s, sys: 2h 32min 45s, total: 9h 32min 2s\n",
      "Wall time: 8h 50min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 에폭 반복\n",
    "for epoch in range(10):\n",
    "    print('Total Epoch :', epoch + 1)\n",
    "\n",
    "    # 훈련 시작\n",
    "    history = model.fit_generator(seq,\n",
    "                                  epochs=100,\n",
    "                                  workers=6,\n",
    "                                  verbose=0)\n",
    "\n",
    "    # 정확도와 손실 출력\n",
    "    print(history.history)\n",
    "    print('accuracy :', history.history['accuracy'][-1])\n",
    "    print('loss :', history.history['loss'][-1])\n",
    "    \n",
    "    # 문장 예측 테스트\n",
    "    # (3 박 4일 놀러 가고 싶다) -> (여행 은 언제나 좋죠)\n",
    "    input_encoder = x_encoder[2].reshape(1, x_encoder[2].shape[0])\n",
    "    input_decoder = x_decoder[2].reshape(1, x_decoder[2].shape[0])\n",
    "    results = model.predict([input_encoder, input_decoder])\n",
    "    \n",
    "    # 결과의 원핫인코딩 형식을 인덱스로 변환\n",
    "    # 1축을 기준으로 가장 높은 값의 위치를 구함\n",
    "    indexs = np.argmax(results[0], 1) \n",
    "    \n",
    "    # 인덱스를 문장으로 변환\n",
    "    sentence = convert_index_to_text(indexs, index_to_word)\n",
    "    print(sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습이 진행될수록 예측 문장이 제대로 생성되는 것을 볼 수 있습니다. 다만 여기서의 예측은 단순히 테스트를 위한 것이라, 인코더 입력과 디코더 입력 데이터가 동시에 사용됩니다. 아래 문장 생성에서는 예측 모델을 적용하기 때문에, 오직 인코더 입력 데이터만 집어 넣습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('test.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# 에폭 반복\n",
    "for epoch in range(20):\n",
    "    print('Total Epoch :', epoch + 1)\n",
    "```\n",
    "> 이 부분이 왜 있는지 모르겠음... 하이퍼 파라미터 튜닝을 위한 것인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측을 위한 입력 생성\n",
    "def make_predict_input(sentence):\n",
    "\n",
    "    sentences = []\n",
    "    sentences.append(sentence)\n",
    "    sentences = pos_tag(sentences)\n",
    "    input_seq = convert_text_to_index(sentences, word_to_index, ENCODER_INPUT)\n",
    "    \n",
    "    return input_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 텍스트 생성\n",
    "def generate_text(input_seq):\n",
    "    \n",
    "    # 입력을 인코더에 넣어 마지막 상태 구함\n",
    "    states = encoder_model.predict(input_seq)\n",
    "\n",
    "    # 목표 시퀀스 초기화\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    \n",
    "    # 목표 시퀀스의 첫 번째에 <START> 태그 추가\n",
    "    target_seq[0, 0] = STA_INDEX\n",
    "    \n",
    "    # 인덱스 초기화\n",
    "    indexs = []\n",
    "    \n",
    "    # 디코더 타임 스텝 반복\n",
    "    while 1:\n",
    "        # 디코더로 현재 타임 스텝 출력 구함\n",
    "        # 처음에는 인코더 상태를, 다음부터 이전 디코더 상태로 초기화\n",
    "        decoder_outputs, state_h, state_c = decoder_model.predict(\n",
    "                                                [target_seq] + states)\n",
    "\n",
    "        # 결과의 원핫인코딩 형식을 인덱스로 변환\n",
    "        index = np.argmax(decoder_outputs[0, 0, :])\n",
    "        indexs.append(index)\n",
    "        \n",
    "        # 종료 검사\n",
    "        if index == END_INDEX or len(indexs) >= max_sequences:\n",
    "            break\n",
    "\n",
    "        # 목표 시퀀스를 바로 이전의 출력으로 설정\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = index\n",
    "        \n",
    "        # 디코더의 이전 상태를 다음 디코더 예측에 사용\n",
    "        states = [state_h, state_c]\n",
    "\n",
    "    # 인덱스를 문장으로 변환\n",
    "    sentence = convert_index_to_text(indexs, index_to_word)\n",
    "        \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "제일 첫 단어는 START로 시작합니다. 그리고 출력으로 나온 인덱스를 디코더 입력으로 넣고 다시 예측을 반복합니다. 상태값을 받아 다시 입력으로 같이 넣는 것에 주의하시기 바랍니다. END 태그가 나오면 문장 생성을 종료합니다.\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 519, 4737, 5679,  239, 1024, 5110, 4109, 5340, 4473, 6085,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장을 인덱스로 변환\n",
    "input_seq = make_predict_input('3박4일 놀러가고 싶다')\n",
    "input_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'여행 은 언제나 좋 죠 '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측 모델로 텍스트 생성\n",
    "sentence = generate_text(input_seq)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋에 있는 문장과 똑같은 입력을 넣으니, 역시 정확히 일치하는 답변이 출력되었습니다.\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 519, 4737, 5679,  239, 6015, 1024, 5110, 4109, 5340, 4473, 6085,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장을 인덱스로 변환\n",
    "input_seq = make_predict_input('3박4일 같이 놀러가고 싶다')\n",
    "input_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'여행 은 언제나 좋 죠 '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측 모델로 텍스트 생성\n",
    "sentence = generate_text(input_seq)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋 문장에서는 없던 '같이'라는 단어를 추가해 보았습니다. 그래도 비슷한 의미란 것을 파악하여 동일한 답변이 나왔습니다.\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 519, 4737, 5679,  239, 1024, 5110, 4109, 6479,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장을 인덱스로 변환\n",
    "input_seq = make_predict_input('3박4일 놀러가려고')\n",
    "input_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'여행 은 언제나 좋 죠 '"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측 모델로 텍스트 생성\n",
    "sentence = generate_text(input_seq)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 데이터셋에 없던 '가려고'로 입력을 수정하니, 전혀 다른 문장이 출력되었습니다. 이는 우리가 데이터의 일부인 100개 문장만 학습했기 때문입니다. 데이터의 개수를 늘려서 훈련할수록 일반화 능력이 더욱 높아집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'제게 기대 세요 '"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"위로 좀 해줘\"\n",
    "input_seq = make_predict_input(input_text)\n",
    "generate_text(input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(input_text):\n",
    "    input_seq = make_predict_input(input_text)\n",
    "    answer = generate_text(input_seq)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'그럼 요 한 순간 에 내 삶 속 에서 내보내 기 는 쉽 지 않 을 거 예요 '"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer(\"살기 힘드네ㅠ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'힘내 세요 '"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer(\"으엥ㅠㅠ 고마워\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
